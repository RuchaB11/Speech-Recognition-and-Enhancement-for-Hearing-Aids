{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da294f-20e3-4246-ad91-1972ad79ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bffac-39df-4e22-9465-6b177fbb2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Input, Concatenate, Layer, Conv1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Audio, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from IPython.display import Audio, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca25ae-6f9a-4a77-a566-a8a6bb9860ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "output_dir = \"improved_hearing_aid_results\"\n",
    "models_dir = \"improved_hearing_aid_models\"\n",
    "test_audio_dir = \"test_audio_samples\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(test_audio_dir, exist_ok=True)\n",
    "\n",
    "audio_files_dir = r\"C:\\Users\\rucha\\.cache\\kagglehub\\datasets\\muhmagdy\\valentini-noisy\\versions\\3\"\n",
    "train_clean_dir = os.path.join(audio_files_dir, 'clean_trainset_28spk_wav')\n",
    "train_noisy_dir = os.path.join(audio_files_dir, 'noisy_trainset_28spk_wav')\n",
    "test_clean_dir = os.path.join(audio_files_dir, 'clean_testset_wav')\n",
    "test_noisy_dir = os.path.join(audio_files_dir, 'noisy_testset_wav')\n",
    "\n",
    "print(\"Dataset directories:\")\n",
    "print(f\"Train clean: {train_clean_dir}\")\n",
    "print(f\"Train noisy: {train_noisy_dir}\")\n",
    "print(f\"Test clean: {test_clean_dir}\")\n",
    "print(f\"Test noisy: {test_noisy_dir}\")\n",
    "\n",
    "HEARING_LOSS_PROFILES = {\n",
    "    'mild': [5, 10, 15, 20, 25],                Mild loss\n",
    "    'moderate': [10, 20, 35, 45, 50],           Moderate loss\n",
    "    'severe': [20, 35, 55, 70, 80],             Severe loss\n",
    "    'high_freq': [0, 5, 15, 35, 60],            High frequency loss (most common)\n",
    "    'cookie_bite': [15, 30, 40, 30, 20],        Mid-frequency loss\n",
    "    'reverse_slope': [45, 35, 25, 15, 5],       Low-frequency loss (reverse slope)\n",
    "    'flat': [30, 30, 30, 30, 30]                Flat loss\n",
    "}\n",
    "\n",
    "FREQ_BANDS = [0, 500, 1000, 2000, 4000, 8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6f402-8e08-4a57-8a84-d00de50214a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    def __init__(self):\n",
    "        self.fs = 10000  \n",
    "        self.gridcoarseness = 1\n",
    "\n",
    "def thirdoct(fs, nfft, num_bands, min_freq):\n",
    "    f = np.linspace(0, fs, nfft + 1)\n",
    "    f = f[:nfft//2 + 1]\n",
    "    \n",
    "    k = np.arange(num_bands)\n",
    "    cf = min_freq * 2**(k/3)\n",
    "    \n",
    "    freq_low = min_freq * 2**((k-0.5)/3)\n",
    "    freq_high = min_freq * 2**((k+0.5)/3)\n",
    "    \n",
    "    obm = np.zeros((num_bands, len(f)))\n",
    "    \n",
    "    for i in range(len(cf)):\n",
    "        k1 = np.argmin((f - freq_low[i])**2)\n",
    "        k2 = np.argmin((f - freq_high[i])**2)\n",
    "        \n",
    "        if k2 > k1:\n",
    "            obm[i, k1:k2] = 1 / (k2 - k1)\n",
    "    \n",
    "    fids = []\n",
    "    for i in range(num_bands):\n",
    "        f_low = min_freq * 2**((i-0.5)/3)\n",
    "        f_high = min_freq * 2**((i+0.5)/3)\n",
    "        \n",
    "        idx = np.where((f >= f_low) & (f <= f_high))[0]\n",
    "        if len(idx) > 0:\n",
    "            fids.append([idx[0] + 1, idx[-1] + 1])\n",
    "    \n",
    "    fids = np.array(fids)\n",
    "    \n",
    "    return obm, cf, fids, freq_low, freq_high\n",
    "\n",
    "class MBSTOI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.constants = Constants()\n",
    "        \n",
    "    def _signal_to_frames(self, x, framelen, hop):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        frames = []\n",
    "        for i in range(0, x.shape[1] - framelen + 1, hop):\n",
    "            frames.append(x[:, i:i+framelen])\n",
    "            \n",
    "        if len(frames) > 0:\n",
    "            frames = torch.stack(frames, dim=1)\n",
    "            window = torch.hann_window(framelen, device=x.device)\n",
    "            frames = frames * window.unsqueeze(0).unsqueeze(0)\n",
    "            return frames\n",
    "        else:\n",
    "            return torch.zeros((x.shape[0], 0, framelen), device=x.device)\n",
    "    \n",
    "    def _detect_silent_frames(self, x, dyn_range, framelen, hop):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            \n",
    "        frames = self._signal_to_frames(x, framelen, hop)\n",
    "        \n",
    "        energy = 10 * torch.log10(torch.sum(frames**2, dim=2) + 1e-8)\n",
    "        \n",
    "        max_energy = torch.max(energy, dim=1, keepdim=True)[0]\n",
    "        \n",
    "        mask = energy > (max_energy - dyn_range)\n",
    "        \n",
    "        return mask, frames\n",
    "            \n",
    "    def _remove_silent_frames(self, xl, xr, yl, yr, dyn_range, N_frame, hop):\n",
    "        if not torch.is_tensor(xl):\n",
    "            xl = torch.tensor(xl, dtype=torch.float32)\n",
    "            xr = torch.tensor(xr, dtype=torch.float32)\n",
    "            yl = torch.tensor(yl, dtype=torch.float32)\n",
    "            yr = torch.tensor(yr, dtype=torch.float32)\n",
    "            \n",
    "        mask_xl, frames_xl = self._detect_silent_frames(xl, dyn_range, N_frame, hop)\n",
    "        mask_xr, frames_xr = self._detect_silent_frames(xr, dyn_range, N_frame, hop)\n",
    "        \n",
    "        mask = torch.logical_or(mask_xl, mask_xr).squeeze()\n",
    "        \n",
    "        frames_yl = self._signal_to_frames(yl, N_frame, hop)\n",
    "        frames_yr = self._signal_to_frames(yr, N_frame, hop)\n",
    "        \n",
    "        frames_xl_active = frames_xl[:, mask, :]\n",
    "        frames_xr_active = frames_xr[:, mask, :]\n",
    "        frames_yl_active = frames_yl[:, mask, :]\n",
    "        frames_yr_active = frames_yr[:, mask, :]\n",
    "        \n",
    "        return frames_xl_active, frames_xr_active, frames_yl_active, frames_yr_active\n",
    "        \n",
    "    def stoi_measure(self, x_frames, y_frames):\n",
    "        if x_frames.shape[1] == 0:\n",
    "            return torch.tensor(0.0, device=x_frames.device)\n",
    "            \n",
    "        batch_size, num_frames, frame_len = x_frames.shape\n",
    "        \n",
    "        corrs = []\n",
    "        \n",
    "        for i in range(num_frames):\n",
    "            x_frame = x_frames[:, i, :]\n",
    "            y_frame = y_frames[:, i, :]\n",
    "            \n",
    "            x_norm = x_frame - torch.mean(x_frame, dim=1, keepdim=True)\n",
    "            y_norm = y_frame - torch.mean(y_frame, dim=1, keepdim=True)\n",
    "            \n",
    "            num = torch.sum(x_norm * y_norm, dim=1)\n",
    "            denom = torch.sqrt(torch.sum(x_norm**2, dim=1) * torch.sum(y_norm**2, dim=1) + 1e-8)\n",
    "            corr = num / denom\n",
    "            \n",
    "            corrs.append(corr)\n",
    "            \n",
    "        if len(corrs) > 0:\n",
    "            corrs = torch.stack(corrs, dim=1)\n",
    "            return torch.mean(corrs)\n",
    "        else:\n",
    "            return torch.tensor(0.0, device=x_frames.device)\n",
    "    \n",
    "    def forward(self, xl, xr, yl, yr):\n",
    "        fs = 10000  \n",
    "        N_frame = 256  \n",
    "        hop = N_frame // 2  \n",
    "        dyn_range = 40  \n",
    "        \n",
    "        if not torch.is_tensor(xl):\n",
    "            xl = torch.tensor(xl, dtype=torch.float32)\n",
    "            xr = torch.tensor(xr, dtype=torch.float32)\n",
    "            yl = torch.tensor(yl, dtype=torch.float32)\n",
    "            yr = torch.tensor(yr, dtype=torch.float32)\n",
    "            \n",
    "        xl = xl.reshape(-1)\n",
    "        xr = xr.reshape(-1)\n",
    "        yl = yl.reshape(-1)\n",
    "        yr = yr.reshape(-1)\n",
    "        \n",
    "        max_len = 32000  \n",
    "        if xl.shape[0] > max_len:\n",
    "            xl = xl[:max_len]\n",
    "            xr = xr[:max_len]\n",
    "            yl = yl[:max_len]\n",
    "            yr = yr[:max_len]\n",
    "            \n",
    "        min_len = min(xl.shape[0], xr.shape[0], yl.shape[0], yr.shape[0])\n",
    "        xl = xl[:min_len].unsqueeze(0)  \n",
    "        xr = xr[:min_len].unsqueeze(0)\n",
    "        yl = yl[:min_len].unsqueeze(0)\n",
    "        yr = yr[:min_len].unsqueeze(0)\n",
    "        \n",
    "        xl_frames, xr_frames, yl_frames, yr_frames = self._remove_silent_frames(\n",
    "            xl, xr, yl, yr, dyn_range, N_frame, hop\n",
    "        )\n",
    "        \n",
    "        stoi_l = self.stoi_measure(xl_frames, yl_frames)\n",
    "        stoi_r = self.stoi_measure(xr_frames, yr_frames)\n",
    "        \n",
    "        mbstoi = torch.maximum(stoi_l, stoi_r)\n",
    "        \n",
    "        return mbstoi\n",
    "\n",
    "def evaluate_mbstoi(clean_left, clean_right, processed_left, processed_right):\n",
    "    mbstoi = MBSTOI()\n",
    "    \n",
    "    xl = torch.tensor(clean_left, dtype=torch.float32)\n",
    "    xr = torch.tensor(clean_right, dtype=torch.float32)\n",
    "    yl = torch.tensor(processed_left, dtype=torch.float32)\n",
    "    yr = torch.tensor(processed_right, dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        score = mbstoi(xl, xr, yl, yr)\n",
    "        \n",
    "    return score.item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be1737-3bd5-4f06-add0-0d3098633e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_safely(audio, sr, low_freq, high_freq):\n",
    "    if low_freq <= 0:  \n",
    "        low_freq = None\n",
    "        high_norm = min(high_freq / (sr/2), 0.99)\n",
    "        ftype = 'lowpass'\n",
    "        freq = high_norm\n",
    "    elif high_freq >= sr/2:  \n",
    "        high_freq = None\n",
    "        low_norm = min(low_freq / (sr/2), 0.99)\n",
    "        ftype = 'highpass'\n",
    "        freq = low_norm\n",
    "    else:  \n",
    "        low_norm = min(low_freq / (sr/2), 0.99)\n",
    "        high_norm = min(high_freq / (sr/2), 0.99)\n",
    "        ftype = 'bandpass'\n",
    "        freq = [low_norm, high_norm]\n",
    "    \n",
    "    try:\n",
    "        b, a = signal.butter(2, freq, btype=ftype)\n",
    "        filtered = signal.filtfilt(b, a, audio)\n",
    "        if not np.all(np.isfinite(filtered)):\n",
    "            filtered = np.nan_to_num(filtered)\n",
    "        return filtered\n",
    "    except Exception as e:\n",
    "        print(f\"Filtering error: {e} - Returning zeros\")\n",
    "        return np.zeros_like(audio)\n",
    "\n",
    "def extract_band_energies(audio, sr):\n",
    "    band_signals = []\n",
    "    band_energies = []\n",
    "    \n",
    "    for i in range(len(FREQ_BANDS)-1):\n",
    "        low_freq = FREQ_BANDS[i]\n",
    "        high_freq = FREQ_BANDS[i+1]\n",
    "        \n",
    "        band_signal = apply_filter_safely(audio, sr, low_freq, high_freq)\n",
    "        band_signals.append(band_signal)\n",
    "        \n",
    "        energy = np.sqrt(np.mean(band_signal**2))\n",
    "        band_energies.append(energy)\n",
    "    \n",
    "    return np.array(band_signals), np.array(band_energies)\n",
    "\n",
    "def apply_hearing_loss(audio, sr, loss_profile='high_freq'):\n",
    "    if isinstance(loss_profile, str):\n",
    "        loss_db = HEARING_LOSS_PROFILES[loss_profile]\n",
    "    else:\n",
    "        loss_db = loss_profile  \n",
    "    \n",
    "     Split audio into frequency bands\n",
    "    band_signals = []\n",
    "    for i in range(len(FREQ_BANDS)-1):\n",
    "        low_freq = FREQ_BANDS[i]\n",
    "        high_freq = FREQ_BANDS[i+1]\n",
    "        \n",
    "        band_signal = apply_filter_safely(audio, sr, low_freq, high_freq)\n",
    "        band_signals.append(band_signal)\n",
    "    \n",
    "    attenuated_bands = []\n",
    "    for i, band in enumerate(band_signals):\n",
    "        attenuation = 10 ** (-loss_db[i] / 20)\n",
    "        attenuated_bands.append(band * attenuation)\n",
    "    \n",
    "    output = np.sum(attenuated_bands, axis=0)\n",
    "    \n",
    "    max_amp = np.max(np.abs(output))\n",
    "    if max_amp > 0.95:\n",
    "        output = output * (0.95 / max_amp)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ce809-e892-486c-a199-b9f975e54739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(audio, snr_range=(5, 15), noise_type='white'):\n",
    "\n",
    "    if noise_type == 'white':\n",
    "        noise = np.random.normal(0, 1, size=audio.shape)\n",
    "    elif noise_type == 'pink':\n",
    "        white_noise = np.random.normal(0, 1, size=audio.shape)\n",
    "        b = [0.049922035, -0.095993537, 0.050612699, -0.004408786]\n",
    "        a = [1, -2.494956002, 2.017265875, -0.522189400]\n",
    "        noise = signal.lfilter(b, a, white_noise)\n",
    "    elif noise_type == 'speech_shaped':\n",
    "        white_noise = np.random.normal(0, 1, size=audio.shape)\n",
    "        lpc_order = 12\n",
    "        try:\n",
    "            a = signal.lpc(audio, lpc_order)[0]\n",
    "            noise = signal.lfilter([1], a, white_noise)\n",
    "        except:\n",
    "            noise = signal.lfilter([1], [1, -0.95], white_noise)\n",
    "    elif noise_type == 'babble':\n",
    "        noise = np.zeros_like(audio)\n",
    "        for i in range(6):   Combine 6 shifted copies\n",
    "            shift = np.random.randint(1000, 10000)\n",
    "            noise += np.roll(audio, shift) * np.random.uniform(0.1, 0.3)\n",
    "        noise = noise - np.mean(noise * audio) * audio / np.mean(audio**2)\n",
    "    else:\n",
    "        noise = np.random.normal(0, 1, size=audio.shape)\n",
    "    \n",
    "    signal_power = np.mean(audio**2)\n",
    "    noise_power = np.mean(noise**2)\n",
    "    \n",
    "    snr = np.random.uniform(snr_range[0], snr_range[1])\n",
    "    noise_scale = np.sqrt(signal_power / (noise_power * 10**(snr/10)))\n",
    "    \n",
    "    noisy_audio = audio + noise * noise_scale\n",
    "    \n",
    "    max_amp = np.max(np.abs(noisy_audio))\n",
    "    if max_amp > 0.95:\n",
    "        noisy_audio = noisy_audio * (0.95 / max_amp)\n",
    "    \n",
    "    return noisy_audio\n",
    "\n",
    "def add_reverberation(audio, sr, rt60_range=(0.2, 0.8)):\n",
    "\n",
    "    rt60 = np.random.uniform(rt60_range[0], rt60_range[1])\n",
    "    \n",
    "    n_samples = int(rt60 * sr)\n",
    "    decay = np.exp(-6.91 * np.arange(n_samples) / n_samples)\n",
    "    \n",
    "    impulse_response = np.random.randn(n_samples) * decay\n",
    "    impulse_response[0] = 1.0  \n",
    "    \n",
    "    impulse_response = impulse_response / np.sum(np.abs(impulse_response))\n",
    "    \n",
    "    reverb_audio = signal.convolve(audio, impulse_response)[:len(audio)]\n",
    "    \n",
    "    max_amp = np.max(np.abs(reverb_audio))\n",
    "    if max_amp > 0.95:\n",
    "        reverb_audio = reverb_audio * (0.95 / max_amp)\n",
    "    \n",
    "    return reverb_audio\n",
    "\n",
    "def apply_channel_effects(audio, sr):\n",
    "\n",
    "    band_signals, _ = extract_band_energies(audio, sr)\n",
    "    eq_gains = np.random.uniform(0.7, 1.3, size=len(band_signals))\n",
    "    \n",
    "    eq_audio = np.zeros_like(audio)\n",
    "    for i, band in enumerate(band_signals):\n",
    "        eq_audio += band * eq_gains[i]\n",
    "    \n",
    "    max_amp = np.max(np.abs(eq_audio))\n",
    "    if max_amp > 0.95:\n",
    "        eq_audio = eq_audio * (0.95 / max_amp)\n",
    "    \n",
    "    return eq_audio\n",
    "\n",
    "def create_challenging_audio(clean_audio, sr):\n",
    "\n",
    "    effects = np.random.choice([\n",
    "        'noise', 'reverb', 'noise+reverb', 'channel', 'all'\n",
    "    ])\n",
    "    \n",
    "    audio = clean_audio.copy()\n",
    "    \n",
    "    if 'noise' in effects:\n",
    "        noise_type = np.random.choice(['white', 'pink', 'speech_shaped', 'babble'])\n",
    "        audio = add_noise(audio, snr_range=(3, 10), noise_type=noise_type)\n",
    "    \n",
    "    if 'reverb' in effects:\n",
    "        audio = add_reverberation(audio, sr, rt60_range=(0.3, 1.0))\n",
    "    \n",
    "    if 'channel' in effects or 'all' in effects:\n",
    "        audio = apply_channel_effects(audio, sr)\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e271bd-0376-4217-a645-ac01df03654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_diverse_binaural_data(file_list, file_dir, hearing_profiles=None, \n",
    "                                 max_files=20, challenging_audio=True):\n",
    "\n",
    "    if hearing_profiles is None:\n",
    "        hearing_profiles = list(HEARING_LOSS_PROFILES.keys())\n",
    "    \n",
    "    X_left = []  \n",
    "    X_right = []  \n",
    "    y_left = []  \n",
    "    y_right = []  \n",
    "    \n",
    "    sample_audios = []\n",
    "    \n",
    "    print(f\"Processing files with diverse hearing profiles and conditions...\")\n",
    "    \n",
    "    for i, file_name in enumerate(file_list):\n",
    "        if i >= max_files:\n",
    "            break\n",
    "            \n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processing file {i+1}/{min(len(file_list), max_files)}\")\n",
    "        \n",
    "        try:\n",
    "            file_path = os.path.join(file_dir, file_name)\n",
    "            audio, sr = sf.read(file_path)\n",
    "            \n",
    "            if not np.all(np.isfinite(audio)):\n",
    "                print(f\"Warning: File {file_name} contains invalid values. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            for profile in hearing_profiles:\n",
    "                profile_losses = HEARING_LOSS_PROFILES[profile]\n",
    "                profile_gains = [1 + (loss/20) for loss in profile_losses]\n",
    "                \n",
    "                audio_left = audio.copy()\n",
    "                audio_right = np.roll(audio, 4) * 0.8  \n",
    "                \n",
    "                if challenging_audio:\n",
    "                    if np.random.random() < 0.7:  \n",
    "                        audio_left = create_challenging_audio(audio_left, sr)\n",
    "                        audio_right = create_challenging_audio(audio_right, sr)\n",
    "                \n",
    "                audio_left_with_loss = apply_hearing_loss(audio_left, sr, profile)\n",
    "                audio_right_with_loss = apply_hearing_loss(audio_right, sr, profile)\n",
    "                \n",
    "                if len(sample_audios) < 10 and i < 10:\n",
    "                    max_len = min(len(audio_left), 3 * sr)\n",
    "                    sample_audios.append({\n",
    "                        'clean_left': audio_left[:max_len],\n",
    "                        'clean_right': audio_right[:max_len],\n",
    "                        'noisy_left': audio_left_with_loss[:max_len],\n",
    "                        'noisy_right': audio_right_with_loss[:max_len],\n",
    "                        'sr': sr,\n",
    "                        'profile': profile\n",
    "                    })\n",
    "                \n",
    "                frame_length = int(0.03 * sr)  \n",
    "                hop_length = int(0.015 * sr)   \n",
    "                \n",
    "                for start in range(0, len(audio) - frame_length, hop_length):\n",
    "                    left_frame = audio_left_with_loss[start:start+frame_length]\n",
    "                    right_frame = audio_right_with_loss[start:start+frame_length]\n",
    "                    \n",
    "                    _, left_frame_energies = extract_band_energies(left_frame, sr)\n",
    "                    _, right_frame_energies = extract_band_energies(right_frame, sr)\n",
    "                    \n",
    "                    if (len(left_frame_energies) == len(profile_gains) and \n",
    "                        len(right_frame_energies) == len(profile_gains)):\n",
    "                        X_left.append(left_frame_energies)\n",
    "                        X_right.append(right_frame_energies)\n",
    "                        y_left.append(profile_gains)\n",
    "                        y_right.append(profile_gains)\n",
    "                    \n",
    "                    if len(X_left) % 200 == 0 and len(X_left) > 0:\n",
    "                        break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(X_left) == 0:\n",
    "        raise ValueError(\"No valid data extracted from files.\")\n",
    "    \n",
    "    X_left = np.array(X_left)\n",
    "    X_right = np.array(X_right)\n",
    "    y_left = np.array(y_left)\n",
    "    y_right = np.array(y_right)\n",
    "    \n",
    "    X_left = X_left.reshape(X_left.shape[0], 1, X_left.shape[1])\n",
    "    X_right = X_right.reshape(X_right.shape[0], 1, X_right.shape[1])\n",
    "    \n",
    "    print(f\"Prepared {X_left.shape[0]} diverse binaural examples with {X_left.shape[2]} features\")\n",
    "    return X_left, X_right, y_left, y_right, sample_audios\n",
    "\n",
    "def enhance_audio_frame(audio_frame_left, audio_frame_right, sr, model):\n",
    "\n",
    "    left_bands, left_energies = extract_band_energies(audio_frame_left, sr)\n",
    "    right_bands, right_energies = extract_band_energies(audio_frame_right, sr)\n",
    "    \n",
    "    left_input = left_energies.reshape(1, 1, -1)\n",
    "    right_input = right_energies.reshape(1, 1, -1)\n",
    "    \n",
    "    left_gains, right_gains = model.predict([left_input, right_input], verbose=0)\n",
    "    left_gains = left_gains[0]\n",
    "    right_gains = right_gains[0]\n",
    "    \n",
    "    left_enhanced = np.zeros_like(audio_frame_left)\n",
    "    right_enhanced = np.zeros_like(audio_frame_right)\n",
    "    \n",
    "    for i in range(len(FREQ_BANDS)-1):\n",
    "        left_enhanced += left_bands[i] * left_gains[i]\n",
    "        right_enhanced += right_bands[i] * right_gains[i]\n",
    "    \n",
    "    return left_enhanced, right_enhanced\n",
    "\n",
    "def enhance_binaural_audio(left_audio, right_audio, sr, model):\n",
    "\n",
    "    enhanced_left = np.zeros_like(left_audio)\n",
    "    enhanced_right = np.zeros_like(right_audio)\n",
    "    \n",
    "    frame_length = int(0.03 * sr)  \n",
    "    hop_length = int(0.015 * sr)   \n",
    "    window = np.hanning(frame_length)\n",
    "    \n",
    "    left_count = np.zeros_like(left_audio)\n",
    "    right_count = np.zeros_like(right_audio)\n",
    "    \n",
    "    for start in range(0, min(len(left_audio), len(right_audio)) - frame_length, hop_length):\n",
    "        left_frame = left_audio[start:start+frame_length]\n",
    "        right_frame = right_audio[start:start+frame_length]\n",
    "        \n",
    "        left_enhanced, right_enhanced = enhance_audio_frame(left_frame, right_frame, sr, model)\n",
    "        \n",
    "        left_enhanced = left_enhanced * window\n",
    "        right_enhanced = right_enhanced * window\n",
    "        \n",
    "        enhanced_left[start:start+frame_length] += left_enhanced\n",
    "        enhanced_right[start:start+frame_length] += right_enhanced\n",
    "        \n",
    "        left_count[start:start+frame_length] += window\n",
    "        right_count[start:start+frame_length] += window\n",
    "    \n",
    "\n",
    "    left_count[left_count < 0.001] = 1.0\n",
    "    right_count[right_count < 0.001] = 1.0\n",
    "    enhanced_left = enhanced_left / left_count\n",
    "    enhanced_right = enhanced_right / right_count\n",
    "    \n",
    "    left_max = np.max(np.abs(enhanced_left))\n",
    "    if left_max > 0.95:\n",
    "        enhanced_left = enhanced_left * (0.95 / left_max)\n",
    "        \n",
    "    right_max = np.max(np.abs(enhanced_right))\n",
    "    if right_max > 0.95:\n",
    "        enhanced_right = enhanced_right * (0.95 / right_max)\n",
    "    \n",
    "    return enhanced_left, enhanced_right\n",
    "\n",
    "def evaluate_model_with_mbstoi(model, sample_audio):\n",
    "    if not sample_audio:\n",
    "        print(\"No sample audio available for MBSTOI evaluation\")\n",
    "        return None\n",
    "        \n",
    "    mbstoi_scores_noisy = []\n",
    "    mbstoi_scores_enhanced = []\n",
    "    profiles = []\n",
    "    \n",
    "    for sample in sample_audio:\n",
    "        clean_left = sample['clean_left']\n",
    "        clean_right = sample['clean_right']\n",
    "        noisy_left = sample['noisy_left']\n",
    "        noisy_right = sample['noisy_right']\n",
    "        sr = sample['sr']\n",
    "        profile = sample.get('profile', 'unknown')\n",
    "        \n",
    "        enhanced_left, enhanced_right = enhance_binaural_audio(\n",
    "            noisy_left, noisy_right, sr, model\n",
    "        )\n",
    "        \n",
    "        mbstoi_noisy = evaluate_mbstoi(clean_left, clean_right, noisy_left, noisy_right)\n",
    "        mbstoi_enhanced = evaluate_mbstoi(clean_left, clean_right, enhanced_left, enhanced_right)\n",
    "        \n",
    "        mbstoi_scores_noisy.append(mbstoi_noisy)\n",
    "        mbstoi_scores_enhanced.append(mbstoi_enhanced)\n",
    "        profiles.append(profile)\n",
    "    \n",
    "    avg_noisy = np.mean(mbstoi_scores_noisy)\n",
    "    avg_enhanced = np.mean(mbstoi_scores_enhanced)\n",
    "    improvement = avg_enhanced - avg_noisy\n",
    "    \n",
    "    print(f\"MBSTOI Evaluation - Noisy: {avg_noisy:.4f}, Enhanced: {avg_enhanced:.4f}, Improvement: {improvement:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed per-sample results:\")\n",
    "    for i in range(len(mbstoi_scores_noisy)):\n",
    "        imp = mbstoi_scores_enhanced[i] - mbstoi_scores_noisy[i]\n",
    "        print(f\"  Sample {i+1} ({profiles[i]}): Noisy={mbstoi_scores_noisy[i]:.4f}, Enhanced={mbstoi_scores_enhanced[i]:.4f}, Imp={imp:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'noisy': avg_noisy,\n",
    "        'enhanced': avg_enhanced,\n",
    "        'improvement': improvement,\n",
    "        'noisy_scores': mbstoi_scores_noisy,\n",
    "        'enhanced_scores': mbstoi_scores_enhanced,\n",
    "        'profiles': profiles,\n",
    "        'individual_improvements': [e - n for e, n in zip(mbstoi_scores_enhanced, mbstoi_scores_noisy)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7aea5-b2db-499f-93d2-c6b958477401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale=3.0, **kwargs):\n",
    "        super(ScaleLayer, self).__init__(**kwargs)\n",
    "        self.scale = scale\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scale\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(ScaleLayer, self).get_config()\n",
    "        config.update({\"scale\": self.scale})\n",
    "        return config\n",
    "\n",
    "def build_binaural_model(input_shape, num_bands, model_type='lstm'):\n",
    "\n",
    "    left_input = Input(shape=input_shape, name='left_input')\n",
    "    right_input = Input(shape=input_shape, name='right_input')\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        shared_lstm = Bidirectional(LSTM(64, return_sequences=False))\n",
    "        \n",
    "        left_lstm = shared_lstm(left_input)\n",
    "        left_lstm = Dropout(0.3)(left_lstm)\n",
    "        \n",
    "        right_lstm = shared_lstm(right_input)\n",
    "        right_lstm = Dropout(0.3)(right_lstm)\n",
    "        \n",
    "    elif model_type == 'cnn_lstm':\n",
    "\n",
    "        shared_conv = Conv1D(32, 3, activation='relu', padding='same')\n",
    "        shared_lstm = Bidirectional(LSTM(64, return_sequences=False))\n",
    "        \n",
    "        left_conv = shared_conv(left_input)\n",
    "        left_lstm = shared_lstm(left_conv)\n",
    "        left_lstm = Dropout(0.3)(left_lstm)\n",
    "        \n",
    "        right_conv = shared_conv(right_input)\n",
    "        right_lstm = shared_lstm(right_conv)\n",
    "        right_lstm = Dropout(0.3)(right_lstm)\n",
    "        \n",
    "    elif model_type == 'deep_lstm':\n",
    "        shared_lstm1 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        shared_lstm2 = Bidirectional(LSTM(64, return_sequences=False))\n",
    "        \n",
    "        left_lstm1 = shared_lstm1(left_input)\n",
    "        left_lstm1 = Dropout(0.3)(left_lstm1)\n",
    "        left_lstm = shared_lstm2(left_lstm1)\n",
    "        left_lstm = Dropout(0.3)(left_lstm)\n",
    "        \n",
    "        right_lstm1 = shared_lstm1(right_input)\n",
    "        right_lstm1 = Dropout(0.3)(right_lstm1)\n",
    "        right_lstm = shared_lstm2(right_lstm1)\n",
    "        right_lstm = Dropout(0.3)(right_lstm)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    combined = Concatenate()([left_lstm, right_lstm])\n",
    "    shared_dense = Dense(128, activation='relu')(combined)\n",
    "    shared_dense = Dropout(0.3)(shared_dense)\n",
    "    \n",
    "    left_dense = Dense(64, activation='relu')(shared_dense)\n",
    "    right_dense = Dense(64, activation='relu')(shared_dense)\n",
    "    \n",
    "    left_output = Dense(num_bands, activation='sigmoid')(left_dense)\n",
    "    right_output = Dense(num_bands, activation='sigmoid')(right_dense)\n",
    "    \n",
    "    left_scaled = ScaleLayer(scale=3.0, name='left_scaled')(left_output)\n",
    "    right_scaled = ScaleLayer(scale=3.0, name='right_scaled')(right_output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[left_input, right_input],\n",
    "        outputs=[left_scaled, right_scaled]\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics={'left_scaled': 'mae', 'right_scaled': 'mae'}\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "class MBSTOIEvaluationCallback(Callback):\n",
    "    def __init__(self, sample_audios, eval_frequency=5):\n",
    "        super(MBSTOIEvaluationCallback, self).__init__()\n",
    "        self.sample_audios = sample_audios\n",
    "        self.eval_frequency = eval_frequency\n",
    "        self.mbstoi_history = {\n",
    "            'epoch': [],\n",
    "            'noisy': [],\n",
    "            'enhanced': [],\n",
    "            'improvement': []\n",
    "        }\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.eval_frequency == 0:\n",
    "            print(f\"\\nEvaluating MBSTOI at epoch {epoch + 1}...\")\n",
    "            results = evaluate_model_with_mbstoi(self.model, self.sample_audios)\n",
    "            \n",
    "            if results:\n",
    "                self.mbstoi_history['epoch'].append(epoch + 1)\n",
    "                self.mbstoi_history['noisy'].append(results['noisy'])\n",
    "                self.mbstoi_history['enhanced'].append(results['enhanced'])\n",
    "                self.mbstoi_history['improvement'].append(results['improvement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db0f42-156e-40f9-985f-a4e7679a53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_hearing_aid_model(epochs=200, model_type='lstm', \n",
    "                                    hearing_profiles=None, max_files=1000):\n",
    "\n",
    "    train_files = sorted(os.listdir(train_clean_dir))[:1000]\n",
    "    random.shuffle(train_files)  \n",
    "    \n",
    "    X_left, X_right, y_left, y_right, sample_audios = prepare_diverse_binaural_data(\n",
    "        train_files, \n",
    "        train_clean_dir, \n",
    "        hearing_profiles=hearing_profiles,\n",
    "        max_files=max_files,\n",
    "        challenging_audio=True\n",
    "    )\n",
    "    \n",
    "    X_left_train, X_left_val, X_right_train, X_right_val, y_left_train, y_left_val, y_right_train, y_right_val = train_test_split(\n",
    "        X_left, X_right, y_left, y_right, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training data shapes: Left input {X_left_train.shape}, Right input {X_right_train.shape}\")\n",
    "    \n",
    "    input_shape = (X_left_train.shape[1], X_left_train.shape[2])\n",
    "    num_bands = y_left_train.shape[1]\n",
    "    model = build_binaural_model(input_shape, num_bands, model_type=model_type)\n",
    "    model.summary()\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        os.path.join(models_dir, f'binaural_{model_type}_model.h5'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    mbstoi_eval = MBSTOIEvaluationCallback(\n",
    "        sample_audios=sample_audios,\n",
    "        eval_frequency=5  \n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    history = model.fit(\n",
    "        [X_left_train, X_right_train],\n",
    "        [y_left_train, y_right_train],\n",
    "        validation_data=([X_left_val, X_right_val], [y_left_val, y_right_val]),\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr, mbstoi_eval],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    history_df = pd.DataFrame({\n",
    "        'loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'left_scaled_mae': history.history['left_scaled_mae'],\n",
    "        'right_scaled_mae': history.history['right_scaled_mae'],\n",
    "        'val_left_scaled_mae': history.history['val_left_scaled_mae'],\n",
    "        'val_right_scaled_mae': history.history['val_right_scaled_mae']\n",
    "    })\n",
    "    history_df.to_csv(os.path.join(output_dir, f'training_history_{model_type}.csv'), index=False)\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['left_scaled_mae'], label='Left Ear')\n",
    "    plt.plot(history.history['right_scaled_mae'], label='Right Ear')\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    mbstoi_epochs = mbstoi_eval.mbstoi_history['epoch']\n",
    "    mbstoi_noisy = mbstoi_eval.mbstoi_history['noisy']\n",
    "    mbstoi_enhanced = mbstoi_eval.mbstoi_history['enhanced']\n",
    "    mbstoi_improvement = mbstoi_eval.mbstoi_history['improvement']\n",
    "    \n",
    "    plt.plot(mbstoi_epochs, mbstoi_noisy, 'b-o', label='Noisy')\n",
    "    plt.plot(mbstoi_epochs, mbstoi_enhanced, 'g-o', label='Enhanced')\n",
    "    \n",
    "    for i, epoch in enumerate(mbstoi_epochs):\n",
    "        improvement = mbstoi_improvement[i]\n",
    "        color = 'green' if improvement > 0 else 'red'\n",
    "        plt.annotate(f\"{improvement:.4f}\", \n",
    "                    xy=(epoch, mbstoi_enhanced[i]),\n",
    "                    xytext=(0, 10),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', color=color, fontsize=9)\n",
    "    \n",
    "    plt.title('MBSTOI Intelligibility During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MBSTOI Score')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'training_results_{model_type}.png'))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    return model, mbstoi_eval.mbstoi_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c14748-882a-47fb-9778-fc608bec81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_files_with_hearing_loss():\n",
    "\n",
    "    test_files = sorted(os.listdir(test_clean_dir))[:1000]  # Use 10 test files\n",
    "    \n",
    "\n",
    "    test_profiles = ['mild', 'moderate', 'high_freq', 'severe']\n",
    "    \n",
    "    test_cases = []\n",
    "    \n",
    "    for i, file_name in enumerate(test_files):\n",
    "        try:\n",
    "            file_path = os.path.join(test_clean_dir, file_name)\n",
    "            audio, sr = sf.read(file_path)\n",
    "            \n",
    "            max_len = min(len(audio), 30 * sr)\n",
    "            audio = audio[:max_len]\n",
    "            \n",
    "            for profile in test_profiles:\n",
    "                audio_left = audio.copy()\n",
    "                audio_right = np.roll(audio, 4) * 0.8\n",
    "                \n",
    "                if np.random.random() < 0.7:\n",
    "                    condition = np.random.choice(['noise', 'reverb', 'both'])\n",
    "                    \n",
    "                    if condition == 'noise' or condition == 'both':\n",
    "                        noise_type = np.random.choice(['white', 'pink', 'speech_shaped'])\n",
    "                        snr = np.random.uniform(3, 10)\n",
    "                        audio_left = add_noise(audio_left, snr_range=(snr, snr), noise_type=noise_type)\n",
    "                        audio_right = add_noise(audio_right, snr_range=(snr, snr), noise_type=noise_type)\n",
    "                    \n",
    "                    if condition == 'reverb' or condition == 'both':\n",
    "                        rt60 = np.random.uniform(0.3, 0.8)\n",
    "                        audio_left = add_reverberation(audio_left, sr, rt60_range=(rt60, rt60))\n",
    "                        audio_right = add_reverberation(audio_right, sr, rt60_range=(rt60, rt60))\n",
    "                    \n",
    "                    condition_name = condition\n",
    "                else:\n",
    "                    condition_name = 'clean'\n",
    "                \n",
    "                audio_left_with_loss = apply_hearing_loss(audio_left, sr, profile)\n",
    "                audio_right_with_loss = apply_hearing_loss(audio_right, sr, profile)\n",
    "                \n",
    "                test_id = f\"test{i+1}_{profile}_{condition_name}\"\n",
    "                \n",
    "                test_dir = os.path.join(test_audio_dir, test_id)\n",
    "                os.makedirs(test_dir, exist_ok=True)\n",
    "                \n",
    "                sf.write(os.path.join(test_dir, 'clean_left.wav'), audio_left, sr)\n",
    "                sf.write(os.path.join(test_dir, 'clean_right.wav'), audio_right, sr)\n",
    "                sf.write(os.path.join(test_dir, 'noisy_left.wav'), audio_left_with_loss, sr)\n",
    "                sf.write(os.path.join(test_dir, 'noisy_right.wav'), audio_right_with_loss, sr)\n",
    "                \n",
    "                test_cases.append({\n",
    "                    'id': test_id,\n",
    "                    'profile': profile,\n",
    "                    'condition': condition_name,\n",
    "                    'dir': test_dir,\n",
    "                    'sr': sr\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test file {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"Created {len(test_cases)} test cases in {test_audio_dir}\")\n",
    "    return test_cases\n",
    "\n",
    "def evaluate_on_test_cases(model, test_cases):\n",
    "\n",
    "    results = {\n",
    "        'mbstoi': [],\n",
    "        'gains': [],\n",
    "        'profiles': [],\n",
    "        'conditions': []\n",
    "    }\n",
    "    \n",
    "    for case in test_cases:\n",
    "        test_id = case['id']\n",
    "        profile = case['profile']\n",
    "        condition = case['condition']\n",
    "        test_dir = case['dir']\n",
    "        sr = case['sr']\n",
    "        \n",
    "        print(f\"\\nEvaluating test case: {test_id}\")\n",
    "        \n",
    "        clean_left, _ = sf.read(os.path.join(test_dir, 'clean_left.wav'))\n",
    "        clean_right, _ = sf.read(os.path.join(test_dir, 'clean_right.wav'))\n",
    "        noisy_left, _ = sf.read(os.path.join(test_dir, 'noisy_left.wav'))\n",
    "        noisy_right, _ = sf.read(os.path.join(test_dir, 'noisy_right.wav'))\n",
    "        \n",
    "        enhanced_left, enhanced_right = enhance_binaural_audio(\n",
    "            noisy_left, noisy_right, sr, model\n",
    "        )\n",
    "        \n",
    "        sf.write(os.path.join(test_dir, 'enhanced_left.wav'), enhanced_left, sr)\n",
    "        sf.write(os.path.join(test_dir, 'enhanced_right.wav'), enhanced_right, sr)\n",
    "        \n",
    "        mbstoi_noisy = evaluate_mbstoi(clean_left, clean_right, noisy_left, noisy_right)\n",
    "        mbstoi_enhanced = evaluate_mbstoi(clean_left, clean_right, enhanced_left, enhanced_right)\n",
    "        improvement = mbstoi_enhanced - mbstoi_noisy\n",
    "        \n",
    "        print(f\"  MBSTOI - Noisy: {mbstoi_noisy:.4f}, Enhanced: {mbstoi_enhanced:.4f}, Improvement: {improvement:.4f}\")\n",
    "        \n",
    "        _, noisy_energies = extract_band_energies(noisy_left, sr)\n",
    "        _, enhanced_energies = extract_band_energies(enhanced_left, sr)\n",
    "        \n",
    "        noisy_db = 20 * np.log10(noisy_energies + 1e-10)\n",
    "        enhanced_db = 20 * np.log10(enhanced_energies + 1e-10)\n",
    "        gains_db = enhanced_db - noisy_db\n",
    "        \n",
    "        print(\"  Frequency-Specific Gains (dB):\")\n",
    "        print(f\"  Average gain across bands: {np.mean(gains_db):.2f} dB\")\n",
    "        for i in range(len(FREQ_BANDS)-1):\n",
    "            band = f\"{FREQ_BANDS[i]}-{FREQ_BANDS[i+1]}Hz\"\n",
    "            print(f\"  {band:>12}: {gains_db[i]:.2f} dB gain\")\n",
    "        \n",
    "        results['mbstoi'].append({\n",
    "            'test_id': test_id,\n",
    "            'noisy': mbstoi_noisy,\n",
    "            'enhanced': mbstoi_enhanced,\n",
    "            'improvement': improvement\n",
    "        })\n",
    "        \n",
    "        results['gains'].append({\n",
    "            'test_id': test_id,\n",
    "            'average': np.mean(gains_db),\n",
    "            'per_band': {f\"{FREQ_BANDS[i]}-{FREQ_BANDS[i+1]}Hz\": gains_db[i] \n",
    "                         for i in range(len(gains_db))}\n",
    "        })\n",
    "        \n",
    "        results['profiles'].append(profile)\n",
    "        results['conditions'].append(condition)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_and_plot_results(results):\n",
    "\n",
    "    profiles = results['profiles']\n",
    "    conditions = results['conditions']\n",
    "    mbstoi_data = results['mbstoi']\n",
    "    gains_data = results['gains']\n",
    "    \n",
    "    mbstoi_df = pd.DataFrame({\n",
    "        'Profile': profiles,\n",
    "        'Condition': conditions,\n",
    "        'Noisy': [d['noisy'] for d in mbstoi_data],\n",
    "        'Enhanced': [d['enhanced'] for d in mbstoi_data],\n",
    "        'Improvement': [d['improvement'] for d in mbstoi_data]\n",
    "    })\n",
    "    \n",
    "    profile_summary = mbstoi_df.groupby('Profile').agg({\n",
    "        'Noisy': 'mean',\n",
    "        'Enhanced': 'mean',\n",
    "        'Improvement': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    condition_summary = mbstoi_df.groupby('Condition').agg({\n",
    "        'Noisy': 'mean',\n",
    "        'Enhanced': 'mean',\n",
    "        'Improvement': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    overall_summary = {\n",
    "        'avg_noisy': mbstoi_df['Noisy'].mean(),\n",
    "        'avg_enhanced': mbstoi_df['Enhanced'].mean(),\n",
    "        'avg_improvement': mbstoi_df['Improvement'].mean(),\n",
    "        'median_improvement': mbstoi_df['Improvement'].median(),\n",
    "        'positive_improvement_rate': (mbstoi_df['Improvement'] > 0).mean() * 100\n",
    "    }\n",
    "    \n",
    "    avg_gains = {}\n",
    "    for band in FREQ_BANDS[:-1]:\n",
    "        band_key = f\"{band}-{FREQ_BANDS[FREQ_BANDS.index(band)+1]}Hz\"\n",
    "        avg_gains[band_key] = np.mean([g['per_band'].get(band_key, 0) for g in gains_data])\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    x = np.arange(len(profile_summary))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, profile_summary['Noisy'], width, label='Noisy')\n",
    "    plt.bar(x + width/2, profile_summary['Enhanced'], width, label='Enhanced')\n",
    "    plt.xlabel('Hearing Loss Profile')\n",
    "    plt.ylabel('MBSTOI Score')\n",
    "    plt.title('MBSTOI by Hearing Loss Profile')\n",
    "    plt.xticks(x, profile_summary['Profile'])\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(condition_summary['Condition'], condition_summary['Improvement'])\n",
    "    plt.xlabel('Acoustic Condition')\n",
    "    plt.ylabel('MBSTOI Improvement')\n",
    "    plt.title('Improvement by Acoustic Condition')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    bands = list(avg_gains.keys())\n",
    "    gains = list(avg_gains.values())\n",
    "    plt.bar(bands, gains)\n",
    "    plt.xlabel('Frequency Band')\n",
    "    plt.ylabel('Gain (dB)')\n",
    "    plt.title('Average Frequency-Specific Gains')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'comprehensive_evaluation.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nComprehensive Evaluation Results:\")\n",
    "    print(f\"Overall MBSTOI - Noisy: {overall_summary['avg_noisy']:.4f}, Enhanced: {overall_summary['avg_enhanced']:.4f}\")\n",
    "    print(f\"Average improvement: {overall_summary['avg_improvement']:.4f}\")\n",
    "    print(f\"Positive improvement rate: {overall_summary['positive_improvement_rate']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nMBSTOI by Hearing Loss Profile:\")\n",
    "    for _, row in profile_summary.iterrows():\n",
    "        print(f\"  {row['Profile']:>10}: {row['Improvement']:.4f}\")\n",
    "    \n",
    "    print(\"\\nMBSTOI by Acoustic Condition:\")\n",
    "    for _, row in condition_summary.iterrows():\n",
    "        print(f\"  {row['Condition']:>10}: {row['Improvement']:.4f}\")\n",
    "    \n",
    "    print(\"\\nAverage Frequency-Specific Gains (dB):\")\n",
    "    print(f\"Average gain across bands: {np.mean(list(avg_gains.values())):.2f} dB\")\n",
    "    for band, gain in avg_gains.items():\n",
    "        print(f\"  {band:>12}: {gain:.2f} dB gain\")\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_summary,\n",
    "        'by_profile': profile_summary,\n",
    "        'by_condition': condition_summary,\n",
    "        'avg_gains': avg_gains\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc13db2-0dc6-4f9a-849e-6b43ff8c55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model, mbstoi_history = train_improved_hearing_aid_model(\n",
    "        epochs=200,\n",
    "        model_type='cnn_lstm',  \n",
    "        hearing_profiles=['high_freq', 'moderate', 'mild'],  \n",
    "        max_files=1000 \n",
    "    )\n",
    "    \n",
    "    test_cases = create_test_files_with_hearing_loss()\n",
    "    \n",
    "    results = evaluate_on_test_cases(model, test_cases)\n",
    "    \n",
    "    summary = analyze_and_plot_results(results)\n",
    "    \n",
    "    print(\"\\nImproved hearing aid model training and evaluation complete!\")\n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483814f-1701-4d2c-a59f-5d16b7a7fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale=3.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale = scale\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scale\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"scale\": self.scale})\n",
    "        return cfg\n",
    "\n",
    "models_dir = \"improved_hearing_aid_models\"\n",
    "model_path = os.path.join(models_dir, 'binaural_cnn_lstm_model.h5')\n",
    "model = load_model(\n",
    "    model_path,\n",
    "    custom_objects={'ScaleLayer': ScaleLayer},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "FREQ_BANDS = [0, 500, 1000, 2000, 4000, 8000]\n",
    "def enhance_binaural_audio(left, right, sr, model):\n",
    "    win = int(0.03 * sr)\n",
    "    hop = win // 2\n",
    "    window = np.hanning(win)\n",
    "    outL = np.zeros_like(left); cntL = np.zeros_like(left)\n",
    "    outR = np.zeros_like(right); cntR = np.zeros_like(right)\n",
    "\n",
    "    def extract_feats(x):\n",
    "        bands, en = [], []\n",
    "        for lo, hi in zip(FREQ_BANDS, FREQ_BANDS[1:]):\n",
    "            if lo <= 0:\n",
    "                b, a = signal.butter(2, hi/(sr/2), btype='low')\n",
    "            elif hi >= sr/2:\n",
    "                b, a = signal.butter(2, lo/(sr/2), btype='high')\n",
    "            else:\n",
    "                b, a = signal.butter(2, [lo/(sr/2), hi/(sr/2)], btype='band')\n",
    "            band = signal.filtfilt(b, a, x)\n",
    "            bands.append(band)\n",
    "            en.append(np.sqrt(np.mean(band**2)))\n",
    "        return np.stack(bands), np.array(en)\n",
    "\n",
    "    for i in range(0, len(left) - win, hop):\n",
    "        lseg = left[i:i+win]; rseg = right[i:i+win]\n",
    "        lb, lE = extract_feats(lseg)\n",
    "        rb, rE = extract_feats(rseg)\n",
    "        gL, gR = model.predict([lE.reshape(1,1,-1), rE.reshape(1,1,-1)], verbose=0)\n",
    "        enhL = (lb * gL[0][:,None]).sum(axis=0) * window\n",
    "        enhR = (rb * gR[0][:,None]).sum(axis=0) * window\n",
    "        outL[i:i+win] += enhL; cntL[i:i+win] += window\n",
    "        outR[i:i+win] += enhR; cntR[i:i+win] += window\n",
    "\n",
    "    cntL[cntL<1e-3] = 1; cntR[cntR<1e-3] = 1\n",
    "    outL /= cntL; outR /= cntR\n",
    "    for sig in (outL, outR):\n",
    "        pk = np.max(np.abs(sig))\n",
    "        if pk > 0.95: sig *= 0.95/pk\n",
    "    return outL, outR\n",
    "\n",
    "\n",
    "input_dir  = r\"C:\\Users\\rucha\\OneDrive\\Desktop\\Mac files\\FCE\\Project\\testing noisy files\"\n",
    "output_dir = \"improved_hearing_aid_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2))\n",
    "\n",
    "for wav in glob.glob(os.path.join(input_dir, \"*.wav\")):\n",
    "    fname = os.path.basename(wav)\n",
    "    print(f\"\\n Processing {fname}\")\n",
    "\n",
    "    # read & mono\n",
    "    audio, sr = sf.read(wav)\n",
    "    if audio.ndim == 2:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    # binaural inputs\n",
    "    left  = audio.copy()\n",
    "    right = np.roll(audio, 4) * 0.8\n",
    "\n",
    "    # enhance\n",
    "    L, R = enhance_binaural_audio(left, right, sr, model)\n",
    "    enhanced = (L + R) / 2\n",
    "\n",
    "    # save\n",
    "    out_path = os.path.join(output_dir, f\"enh_{fname}\")\n",
    "    sf.write(out_path, enhanced, sr)\n",
    "    \n",
    "    # metrics\n",
    "    r0 = rms(audio)\n",
    "    r1 = rms(enhanced)\n",
    "    gain_db = 20 * np.log10((r1 + 1e-12)/(r0 + 1e-12))\n",
    "\n",
    "    print(f\"  RMS (Noisy):    {r0:.4f}\")\n",
    "    print(f\"  RMS (Enhanced): {r1:.4f}\")\n",
    "    print(f\"  Gain:           {gain_db:+.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562aa3b-40b6-4ef9-b34b-3dec529cc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import noisereduce as nr\n",
    "except ImportError:\n",
    "    nr = None\n",
    "    display(HTML(\"<p style='color:red;'> noisereduce not installed; install with <code>!pip install noisereduce</code> to enable denoising.</p>\"))\n",
    "\n",
    "def load_and_norm(path):\n",
    "    audio, sr = sf.read(path)\n",
    "    if audio.ndim == 2:\n",
    "        audio = audio.mean(axis=1)\n",
    "    peak = np.max(np.abs(audio)) or 1.0\n",
    "    return audio / peak, sr\n",
    "\n",
    "input_dir = r\"C:\\Users\\rucha\\OneDrive\\Desktop\\Mac files\\FCE\\Project\\testing noisy files\"\n",
    "enh_dir   = \"improved_hearing_aid_results\"\n",
    "\n",
    "for wav_path in glob.glob(os.path.join(input_dir, \"*.wav\")):\n",
    "    fname = os.path.basename(wav_path)\n",
    "    display(HTML(f\"<hr><h2>File: {fname}</h2>\"))\n",
    "\n",
    "    noisy, sr = load_and_norm(wav_path)\n",
    "    display(HTML(\"<h4>Original Noisy</h4>\"))\n",
    "    display(Audio(noisy, rate=sr, normalize=False))\n",
    "\n",
    "    enh_path = os.path.join(enh_dir, f\"enh_{fname}\")\n",
    "    if not os.path.exists(enh_path):\n",
    "        display(HTML(f\"<p style='color:red;'>Enhanced file not found: {enh_path}</p>\"))\n",
    "        continue\n",
    "\n",
    "    enhanced, _ = load_and_norm(enh_path)\n",
    "\n",
    "    if nr:\n",
    "        denh = nr.reduce_noise(y=enhanced, sr=sr)\n",
    "        denh /= (np.max(np.abs(denh)) or 1.0)\n",
    "        display(HTML(\"<h4>Denoised Enhanced</h4>\"))\n",
    "        display(Audio(denh, rate=sr, normalize=False))\n",
    "    else:\n",
    "        display(HTML(\"<p style='color:red;'> noisereduce not available, playing raw enhanced.</p>\"))\n",
    "        display(HTML(\"<h4>Enhanced (raw)</h4>\"))\n",
    "        display(Audio(enhanced, rate=sr, normalize=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb0242-fccd-45f8-b82f-d96a27f24676",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import noisereduce as nr\n",
    "except ImportError:\n",
    "    nr = None\n",
    "    display(HTML(\n",
    "      \"<p style='color:red;'> `noisereduce` not installed; \"\n",
    "      \"install with <code>!pip install noisereduce</code> to enable denoising.</p>\"\n",
    "    ))\n",
    "\n",
    "def load_mono(path):\n",
    "    audio, sr = sf.read(path)\n",
    "    if audio.ndim == 2:\n",
    "        audio = audio.mean(axis=1)\n",
    "    return audio, sr\n",
    "\n",
    "input_dir = r\"C:\\Users\\rucha\\OneDrive\\Desktop\\Mac files\\FCE\\Project\\testing noisy files\"\n",
    "enh_dir   = \"improved_hearing_aid_results\"\n",
    "\n",
    "for wav_path in glob.glob(os.path.join(input_dir, \"*.wav\")):\n",
    "    fname = os.path.basename(wav_path)\n",
    "    display(HTML(f\"<hr><h2>File: {fname}</h2>\"))\n",
    "\n",
    "    noisy, sr = load_mono(wav_path)\n",
    "\n",
    "    enh_path = os.path.join(enh_dir, f\"enh_{fname}\")\n",
    "    if not os.path.exists(enh_path):\n",
    "        display(HTML(f\"<p style='color:red;'>Enhanced file not found: {enh_path}</p>\"))\n",
    "        continue\n",
    "    enhanced, _ = load_mono(enh_path)\n",
    "\n",
    "    if nr:\n",
    "        denoised_enh = nr.reduce_noise(y=enhanced, sr=sr)\n",
    "    else:\n",
    "        denoised_enh = enhanced\n",
    "\n",
    "    items = [\n",
    "        (\"Original Noisy\", noisy),\n",
    "        (\"LSTMEnhanced\", enhanced),\n",
    "        (\"Denoised Enhanced\", denoised_enh)\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(len(items), 2, figsize=(12, 4 * len(items)))\n",
    "    for i, (label, sig) in enumerate(items):\n",
    "        t = np.arange(len(sig)) / sr\n",
    "\n",
    "        # waveform\n",
    "        ax_w = axes[i, 0]\n",
    "        ax_w.plot(t, sig, linewidth=0.5)\n",
    "        ax_w.set_title(f\"{label} Waveform\")\n",
    "        ax_w.set_xlabel(\"Time [s]\")\n",
    "        ax_w.set_ylabel(\"Amplitude\")\n",
    "\n",
    "        # spectrogram\n",
    "        ax_s = axes[i, 1]\n",
    "        ax_s.specgram(sig, NFFT=512, Fs=sr, noverlap=256)\n",
    "        ax_s.set_title(f\"{label} Spectrogram\")\n",
    "        ax_s.set_xlabel(\"Time [s]\")\n",
    "        ax_s.set_ylabel(\"Freq [Hz]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8234cc6-2244-43e5-b61a-8500b451743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEARING_LOSS_PROFILES = {\n",
    "    'mild': [5, 10, 15, 20, 25],               \n",
    "    'moderate': [10, 20, 35, 45, 50],          \n",
    "    'severe': [20, 35, 55, 70, 80],            \n",
    "    'high_freq': [0, 5, 15, 35, 60],          \n",
    "    'cookie_bite': [15, 30, 40, 30, 20],       \n",
    "    'reverse_slope': [45, 35, 25, 15, 5],      \n",
    "    'flat': [30, 30, 30, 30, 30]              \n",
    "}\n",
    "\n",
    "FREQ_BANDS = [0, 500, 1000, 2000, 4000, 8000]\n",
    "x_labels = [f\"{FREQ_BANDS[i]}\" for i in range(len(FREQ_BANDS)-1)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "\n",
    "for i, (profile, loss_values) in enumerate(HEARING_LOSS_PROFILES.items()):\n",
    "    plt.plot(range(len(loss_values)), loss_values, 'o-', \n",
    "             linewidth=2.5, label=profile.replace('_', ' ').title(),\n",
    "             color=colors[i], markersize=8)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.axhspan(0, 25, alpha=0.1, color='green')\n",
    "plt.axhspan(25, 40, alpha=0.1, color='yellow')\n",
    "plt.axhspan(40, 70, alpha=0.1, color='orange')\n",
    "plt.axhspan(70, 90, alpha=0.1, color='red')\n",
    "\n",
    "plt.text(4.5, 12.5, 'MILD', fontsize=9, ha='right', color='green', fontweight='bold')\n",
    "plt.text(4.5, 32.5, 'MODERATE', fontsize=9, ha='right', color='#b5b500', fontweight='bold')\n",
    "plt.text(4.5, 55, 'SEVERE', fontsize=9, ha='right', color='#b27300', fontweight='bold')\n",
    "plt.text(4.5, 75, 'PROFOUND', fontsize=9, ha='right', color='#b30000', fontweight='bold')\n",
    "\n",
    "plt.xticks(range(len(x_labels)), x_labels)\n",
    "plt.xlabel('Frequency (Hz)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Hearing Loss (dB)', fontsize=12, fontweight='bold')\n",
    "plt.title('Hearing Loss Profiles Across Frequency Bands', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.legend(title='Profiles', title_fontsize=12, fontsize=10, loc='lower left')\n",
    "\n",
    "plt.annotate('High-frequency loss is most common,\\nwith greatest impact in 4000-8000Hz range', \n",
    "             xy=(4, 60), xytext=(2.5, 35),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "             fontsize=10, ha='center', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hearing_loss_profiles.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
